{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import BertForSequenceClassification, TrainingArguments, Trainer, AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "import tqdm\n",
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "from transformers.pipelines.base import KeyDataset\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KcBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KcBERT: KoBERT trained from scratch using comments data of naver news\n",
    "model_name = 'beomi/kcbert-base'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-82194d63485bfcca\n",
      "Reusing dataset csv (/home/kdy20401/.cache/huggingface/datasets/csv/default-82194d63485bfcca/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n",
      "100%|██████████| 2/2 [00:00<00:00, 331.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# 1. load dataset which is a bit processed from local \n",
    "base_dir = os.getcwd()\n",
    "train_data = base_dir + '/dataset/cleaned_unsmile_train_v1.0.csv'\n",
    "test_data = base_dir + '/dataset/cleaned_unsmile_test_v1.0.csv'\n",
    "\n",
    "dataset = load_dataset('csv', data_files={'train': train_data, 'test': test_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. load dataset from huggingface.co\n",
    "# hf_dataset = load_dataset('smilegate-ai/kor_unsmile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '개인지칭' label is excluded\n",
    "unsmile_columns = [\"여성/가족\",\"남성\",\"성소수자\",\"인종/국적\",\"연령\",\"지역\",\"종교\",\"기타 혐오\",\"악플/욕설\",\"clean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    tokenized_examples = tokenizer(str(examples[\"문장\"]))\n",
    "    tokenized_examples['labels'] = torch.tensor([examples[col] for col in unsmile_columns], dtype=torch.float)\n",
    "    # multi label classification 학습을 위해선 label이 float 형태로 변형되어야 합니다.\n",
    "    # huggingface datasets 최신 버전에는 'map' 함수에 버그가 있어서 변형이 올바르게 되지 않습니다.\n",
    "    \n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_function at 0x7fcec2dfea60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|██████████| 15003/15003 [00:03<00:00, 3826.24ex/s]\n",
      "100%|██████████| 3737/3737 [00:00<00:00, 4056.34ex/s]\n"
     ]
    }
   ],
   "source": [
    "# tokenize data\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=False)\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    2,  2458, 15751, 24930, 24351, 29278, 17038, 11631,     3]),\n",
       " 'labels': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(unsmile_columns)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "model.config.id2label = {i: label for i, label in zip(range(num_labels), unsmile_columns)}\n",
    "model.config.label2id = {label: i for i, label in zip(range(num_labels), unsmile_columns)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(300, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'여성/가족': 0,\n",
       " '남성': 1,\n",
       " '성소수자': 2,\n",
       " '인종/국적': 3,\n",
       " '연령': 4,\n",
       " '지역': 5,\n",
       " '종교': 6,\n",
       " '기타 혐오': 7,\n",
       " '악플/욕설': 8,\n",
       " 'clean': 9}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(x):\n",
    "    return {\n",
    "        'lrap': label_ranking_average_precision_score(x.label_ids, x.predictions),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_epoch = 15\n",
    "\n",
    "# before feeding batch data to the model, apply padding to each sequences in the batch to fit the tensor size\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"model_output/KcBert\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epoch,\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='lrap',\n",
    "    greater_is_better=True,\n",
    "    optim='adamw_torch'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=args, \n",
    "    train_dataset=tokenized_dataset[\"train\"], \n",
    "    eval_dataset=tokenized_dataset[\"test\"], \n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자. If 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 15003\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 885\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='885' max='885' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [885/885 11:25, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Lrap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.228506</td>\n",
       "      <td>0.781661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.157334</td>\n",
       "      <td>0.860580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.138762</td>\n",
       "      <td>0.874631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.131910</td>\n",
       "      <td>0.877017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.131161</td>\n",
       "      <td>0.877105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.135107</td>\n",
       "      <td>0.875694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.135361</td>\n",
       "      <td>0.877933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.137416</td>\n",
       "      <td>0.875916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.129200</td>\n",
       "      <td>0.140853</td>\n",
       "      <td>0.875959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.129200</td>\n",
       "      <td>0.144887</td>\n",
       "      <td>0.875291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.129200</td>\n",
       "      <td>0.145221</td>\n",
       "      <td>0.872237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.129200</td>\n",
       "      <td>0.146390</td>\n",
       "      <td>0.872598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.129200</td>\n",
       "      <td>0.147287</td>\n",
       "      <td>0.875443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.129200</td>\n",
       "      <td>0.149876</td>\n",
       "      <td>0.871440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.129200</td>\n",
       "      <td>0.148799</td>\n",
       "      <td>0.874279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자. If 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/KcBert/checkpoint-59\n",
      "Configuration saved in model_output/KcBert/checkpoint-59/config.json\n",
      "Model weights saved in model_output/KcBert/checkpoint-59/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KcBert/checkpoint-59/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KcBert/checkpoint-59/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자. If 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/KcBert/checkpoint-118\n",
      "Configuration saved in model_output/KcBert/checkpoint-118/config.json\n",
      "Model weights saved in model_output/KcBert/checkpoint-118/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KcBert/checkpoint-118/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KcBert/checkpoint-118/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자. If 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/KcBert/checkpoint-177\n",
      "Configuration saved in model_output/KcBert/checkpoint-177/config.json\n",
      "Model weights saved in model_output/KcBert/checkpoint-177/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KcBert/checkpoint-177/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KcBert/checkpoint-177/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자. If 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/KcBert/checkpoint-236\n",
      "Configuration saved in model_output/KcBert/checkpoint-236/config.json\n",
      "Model weights saved in model_output/KcBert/checkpoint-236/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KcBert/checkpoint-236/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KcBert/checkpoint-236/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자. If 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/KcBert/checkpoint-295\n",
      "Configuration saved in model_output/KcBert/checkpoint-295/config.json\n",
      "Model weights saved in model_output/KcBert/checkpoint-295/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KcBert/checkpoint-295/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KcBert/checkpoint-295/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자. If 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/KcBert/checkpoint-354\n",
      "Configuration saved in model_output/KcBert/checkpoint-354/config.json\n",
      "Model weights saved in model_output/KcBert/checkpoint-354/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KcBert/checkpoint-354/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KcBert/checkpoint-354/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자. If 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/KcBert/checkpoint-413\n",
      "Configuration saved in model_output/KcBert/checkpoint-413/config.json\n",
      "Model weights saved in model_output/KcBert/checkpoint-413/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KcBert/checkpoint-413/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KcBert/checkpoint-413/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자. If 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/KcBert/checkpoint-472\n",
      "Configuration saved in model_output/KcBert/checkpoint-472/config.json\n",
      "Model weights saved in model_output/KcBert/checkpoint-472/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KcBert/checkpoint-472/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KcBert/checkpoint-472/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자. If 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/KcBert/checkpoint-531\n",
      "Configuration saved in model_output/KcBert/checkpoint-531/config.json\n",
      "Model weights saved in model_output/KcBert/checkpoint-531/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KcBert/checkpoint-531/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KcBert/checkpoint-531/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자. If 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/KcBert/checkpoint-590\n",
      "Configuration saved in model_output/KcBert/checkpoint-590/config.json\n",
      "Model weights saved in model_output/KcBert/checkpoint-590/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KcBert/checkpoint-590/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KcBert/checkpoint-590/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자. If 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/KcBert/checkpoint-649\n",
      "Configuration saved in model_output/KcBert/checkpoint-649/config.json\n",
      "Model weights saved in model_output/KcBert/checkpoint-649/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KcBert/checkpoint-649/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KcBert/checkpoint-649/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자. If 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/KcBert/checkpoint-708\n",
      "Configuration saved in model_output/KcBert/checkpoint-708/config.json\n",
      "Model weights saved in model_output/KcBert/checkpoint-708/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KcBert/checkpoint-708/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KcBert/checkpoint-708/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자. If 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/KcBert/checkpoint-767\n",
      "Configuration saved in model_output/KcBert/checkpoint-767/config.json\n",
      "Model weights saved in model_output/KcBert/checkpoint-767/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KcBert/checkpoint-767/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KcBert/checkpoint-767/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자. If 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/KcBert/checkpoint-826\n",
      "Configuration saved in model_output/KcBert/checkpoint-826/config.json\n",
      "Model weights saved in model_output/KcBert/checkpoint-826/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KcBert/checkpoint-826/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KcBert/checkpoint-826/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자. If 악플/욕설, clean, 개인지칭, 문장, 종교, 인종/국적, 기타 혐오, 연령, 지역, 여성/가족, 남성, 성소수자 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/KcBert/checkpoint-885\n",
      "Configuration saved in model_output/KcBert/checkpoint-885/config.json\n",
      "Model weights saved in model_output/KcBert/checkpoint-885/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KcBert/checkpoint-885/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KcBert/checkpoint-885/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from model_output/KcBert/checkpoint-413 (score: 0.8779333435274042).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=885, training_loss=0.08943302995067531, metrics={'train_runtime': 697.7093, 'train_samples_per_second': 322.548, 'train_steps_per_second': 1.268, 'total_flos': 8370692162021616.0, 'train_loss': 0.08943302995067531, 'epoch': 15.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model_output/KcBert\n",
      "Configuration saved in model_output/KcBert/config.json\n",
      "Model weights saved in model_output/KcBert/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KcBert/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KcBert/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "pipe = TextClassificationPipeline(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    device=0,\n",
    "    return_all_scores=True,\n",
    "    function_to_apply='sigmoid'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': '여성/가족', 'score': 0.027701519429683685}\n",
      "{'label': '남성', 'score': 0.026212995871901512}\n",
      "{'label': '성소수자', 'score': 0.21012935042381287}\n",
      "{'label': '인종/국적', 'score': 0.02318049781024456}\n",
      "{'label': '연령', 'score': 0.023564638569951057}\n",
      "{'label': '지역', 'score': 0.058266717940568924}\n",
      "{'label': '종교', 'score': 0.027729108929634094}\n",
      "{'label': '기타 혐오', 'score': 0.7373677492141724}\n",
      "{'label': '악플/욕설', 'score': 0.053706057369709015}\n",
      "{'label': 'clean', 'score': 0.024462051689624786}\n"
     ]
    }
   ],
   "source": [
    "for result in pipe(\"포괄적차별금지법을 반대합니다.\")[0]:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicated_label(output_labels, min_score):\n",
    "    labels = []\n",
    "    for label in output_labels:\n",
    "        if label['score'] > min_score:\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disabling tokenizer parallelism, we're using DataLoader multithreading already\n",
      "100%|██████████| 3737/3737 [00:36<00:00, 101.63it/s]\n"
     ]
    }
   ],
   "source": [
    "predicated_labels = []\n",
    "\n",
    "for out in tqdm.tqdm(pipe(KeyDataset(dataset['test'], '문장'))):\n",
    "    predicated_labels.append(get_predicated_label(out, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.76      0.80       393\n",
      "           1       0.90      0.84      0.87       340\n",
      "           2       0.86      0.79      0.83       281\n",
      "           3       0.84      0.82      0.83       422\n",
      "           4       0.91      0.82      0.86       146\n",
      "           5       0.91      0.90      0.91       261\n",
      "           6       0.89      0.87      0.88       294\n",
      "           7       0.71      0.35      0.47       134\n",
      "           8       0.68      0.66      0.67       770\n",
      "           9       0.76      0.75      0.76       944\n",
      "\n",
      "   micro avg       0.80      0.76      0.78      3985\n",
      "   macro avg       0.83      0.76      0.79      3985\n",
      "weighted avg       0.80      0.76      0.78      3985\n",
      " samples avg       0.78      0.77      0.77      3985\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(classification_report(tokenized_dataset['test']['labels'], predicated_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52634da84371cba311ea128a5ea7cdc41ff074b781779e754b270ff9f8153cee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
