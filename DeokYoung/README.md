**서버 환경**: linux

**가상환경**: anaconda 4.14.0(python 3.8.13)

**가상환경** 내 패키지: packagelist.txt 확인

**모델 학습**

1. 데이터셋: 기존 unsmile dataset을 cleaning(약 120개 데이터 re-labeling)한 데이터셋
2. 사용 모델: KoBERT, KcBERT(KoBERT를 네이버 댓글 데이터를 사용해서 처음부터 학습한 모델)
    
    
**카테고리별 성능**(f1-score)

||여성/가족|남성|성소수자|인종/국적|연령|지역|종교|기타 혐오|악플/욕설|clean|
|---|---|---|---|---|---|---|---|---|---|---|
|KoBERT|0.74|0.79|0.81|0.77|0.25|0.84|0.87|0.00|0.59|0.70|
|KcBERT|0.80|0.87|0.83|0.83|0.86|0.91|0.88|0.47|0.67|0.76|
|Baseline|0.76|0.85|0.83|0.82|0.83|0.88|0.87|0.30|0.67|0.77|
* Baseline은 스마일 게이트에서 제공한 수치
* KoBERT, KcBERT 모두 15 epoch으로 학습


**Data cleaning 전후 성능(f1-score) 비교**

||여성/가족|남성|성소수자|인종/국적|연령|지역|종교|기타 혐오|악플/욕설|clean|micro avg|macro avg|
|---|---|---|---|---|---|---|---|---|---|---|---|---|
|cleaning 전|0.79|0.84|0.83|0.80|0.82|0.86|0.88|0.00|0.67|0.75|0.76|0.72|
|cleaning 후|0.79|0.83|0.83|0.80|0.84|0.88|0.88|0.00|0.68|0.75|0.77|0.73|
* data cleaning이 성능에 영향을 미치는지 파악하기 위해 KcBERT 모델을 사용하여 5 epoch씩 학습시킨 결과
* f1-score 평균 1% 향상



**Epoch 당 loss 변화 추이**

* 그래프로 보여주자

