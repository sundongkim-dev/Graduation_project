{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import BertForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "\n",
    "import tqdm\n",
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "from transformers.pipelines.base import KeyDataset\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from kobert_tokenizer import KoBERTTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KoBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'skt/kobert-base-v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-82194d63485bfcca\n",
      "Reusing dataset csv (/home/kdy20401/.cache/huggingface/datasets/csv/default-82194d63485bfcca/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n",
      "100%|██████████| 2/2 [00:00<00:00, 300.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# 1. load dataset which is a bit processed from local \n",
    "base_dir = os.getcwd()\n",
    "train_data = base_dir + '/dataset/cleaned_unsmile_train_v1.0.csv'\n",
    "test_data = base_dir + '/dataset/cleaned_unsmile_test_v1.0.csv'\n",
    "\n",
    "dataset = load_dataset('csv', data_files={'train': train_data, 'test': test_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. load dataset from huggingface.co\n",
    "# hf_dataset = load_dataset('smilegate-ai/kor_unsmile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '개인지칭' label is excluded\n",
    "unsmile_columns = [\"여성/가족\",\"남성\",\"성소수자\",\"인종/국적\",\"연령\",\"지역\",\"종교\",\"기타 혐오\",\"악플/욕설\",\"clean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    tokenized_examples = tokenizer(str(examples[\"문장\"]))\n",
    "    tokenized_examples['labels'] = torch.tensor([examples[col] for col in unsmile_columns], dtype=torch.float)\n",
    "    # multi label classification 학습을 위해선 label이 float 형태로 변형되어야 합니다.\n",
    "    # huggingface datasets 최신 버전에는 'map' 함수에 버그가 있어서 변형이 올바르게 되지 않습니다.\n",
    "    \n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n",
      "Parameter 'function'=<function preprocess_function at 0x7f1a98c27f70> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|██████████| 15003/15003 [00:04<00:00, 3045.00ex/s]\n",
      "100%|██████████| 3737/3737 [00:01<00:00, 3180.87ex/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = KoBERTTokenizer.from_pretrained(model_name)\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=False)\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([   2, 3803, 6812, 7794, 2962, 7086, 2923, 5439, 6751, 6855, 6553, 1201,\n",
       "         5400, 3093, 5777, 5591,    3]),\n",
       " 'labels': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at skt/kobert-base-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(unsmile_columns)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=num_labels, # original number of labels of BERT is two(0, 1)\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "model.config.id2label = {i: label for i, label in zip(range(num_labels), unsmile_columns)}\n",
    "model.config.label2id = {label: i for i, label in zip(range(num_labels), unsmile_columns)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'여성/가족': 0,\n",
       " '남성': 1,\n",
       " '성소수자': 2,\n",
       " '인종/국적': 3,\n",
       " '연령': 4,\n",
       " '지역': 5,\n",
       " '종교': 6,\n",
       " '기타 혐오': 7,\n",
       " '악플/욕설': 8,\n",
       " 'clean': 9}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(x):\n",
    "    return {\n",
    "        'lrap': label_ranking_average_precision_score(x.label_ids, x.predictions),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_epoch = 15\n",
    "\n",
    "# before feeding batch data to the model, apply padding to each sequences in the batch to fit the tensor size\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"model_output/KoBert\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epoch,\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='lrap',\n",
    "    greater_is_better=True,\n",
    "    optim='adamw_torch'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=args, \n",
    "    train_dataset=tokenized_dataset[\"train\"], \n",
    "    eval_dataset=tokenized_dataset[\"test\"], \n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역. If clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 15003\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 885\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='885' max='885' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [885/885 11:45, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Lrap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.359525</td>\n",
       "      <td>0.467643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.310708</td>\n",
       "      <td>0.516799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.282674</td>\n",
       "      <td>0.608801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.255755</td>\n",
       "      <td>0.703944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.230431</td>\n",
       "      <td>0.778412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.212253</td>\n",
       "      <td>0.802376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.198704</td>\n",
       "      <td>0.812751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.193038</td>\n",
       "      <td>0.814126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.267800</td>\n",
       "      <td>0.187677</td>\n",
       "      <td>0.818008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.267800</td>\n",
       "      <td>0.183271</td>\n",
       "      <td>0.822842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.267800</td>\n",
       "      <td>0.182287</td>\n",
       "      <td>0.823546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.267800</td>\n",
       "      <td>0.182731</td>\n",
       "      <td>0.823249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.267800</td>\n",
       "      <td>0.181492</td>\n",
       "      <td>0.823089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.267800</td>\n",
       "      <td>0.181005</td>\n",
       "      <td>0.824718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.267800</td>\n",
       "      <td>0.180754</td>\n",
       "      <td>0.824383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역. If clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/KoBert/checkpoint-59\n",
      "Configuration saved in model_output/KoBert/checkpoint-59/config.json\n",
      "Model weights saved in model_output/KoBert/checkpoint-59/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KoBert/checkpoint-59/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KoBert/checkpoint-59/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역. If clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/KoBert/checkpoint-118\n",
      "Configuration saved in model_output/KoBert/checkpoint-118/config.json\n",
      "Model weights saved in model_output/KoBert/checkpoint-118/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KoBert/checkpoint-118/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KoBert/checkpoint-118/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역. If clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/KoBert/checkpoint-177\n",
      "Configuration saved in model_output/KoBert/checkpoint-177/config.json\n",
      "Model weights saved in model_output/KoBert/checkpoint-177/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KoBert/checkpoint-177/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KoBert/checkpoint-177/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역. If clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/KoBert/checkpoint-236\n",
      "Configuration saved in model_output/KoBert/checkpoint-236/config.json\n",
      "Model weights saved in model_output/KoBert/checkpoint-236/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KoBert/checkpoint-236/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KoBert/checkpoint-236/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역. If clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/KoBert/checkpoint-295\n",
      "Configuration saved in model_output/KoBert/checkpoint-295/config.json\n",
      "Model weights saved in model_output/KoBert/checkpoint-295/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KoBert/checkpoint-295/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KoBert/checkpoint-295/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역. If clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/KoBert/checkpoint-354\n",
      "Configuration saved in model_output/KoBert/checkpoint-354/config.json\n",
      "Model weights saved in model_output/KoBert/checkpoint-354/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KoBert/checkpoint-354/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KoBert/checkpoint-354/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역. If clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/KoBert/checkpoint-413\n",
      "Configuration saved in model_output/KoBert/checkpoint-413/config.json\n",
      "Model weights saved in model_output/KoBert/checkpoint-413/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KoBert/checkpoint-413/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KoBert/checkpoint-413/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역. If clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/KoBert/checkpoint-472\n",
      "Configuration saved in model_output/KoBert/checkpoint-472/config.json\n",
      "Model weights saved in model_output/KoBert/checkpoint-472/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KoBert/checkpoint-472/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KoBert/checkpoint-472/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역. If clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/KoBert/checkpoint-531\n",
      "Configuration saved in model_output/KoBert/checkpoint-531/config.json\n",
      "Model weights saved in model_output/KoBert/checkpoint-531/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KoBert/checkpoint-531/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KoBert/checkpoint-531/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역. If clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/KoBert/checkpoint-590\n",
      "Configuration saved in model_output/KoBert/checkpoint-590/config.json\n",
      "Model weights saved in model_output/KoBert/checkpoint-590/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KoBert/checkpoint-590/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KoBert/checkpoint-590/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역. If clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/KoBert/checkpoint-649\n",
      "Configuration saved in model_output/KoBert/checkpoint-649/config.json\n",
      "Model weights saved in model_output/KoBert/checkpoint-649/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KoBert/checkpoint-649/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KoBert/checkpoint-649/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역. If clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/KoBert/checkpoint-708\n",
      "Configuration saved in model_output/KoBert/checkpoint-708/config.json\n",
      "Model weights saved in model_output/KoBert/checkpoint-708/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KoBert/checkpoint-708/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KoBert/checkpoint-708/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역. If clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/KoBert/checkpoint-767\n",
      "Configuration saved in model_output/KoBert/checkpoint-767/config.json\n",
      "Model weights saved in model_output/KoBert/checkpoint-767/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KoBert/checkpoint-767/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KoBert/checkpoint-767/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역. If clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/KoBert/checkpoint-826\n",
      "Configuration saved in model_output/KoBert/checkpoint-826/config.json\n",
      "Model weights saved in model_output/KoBert/checkpoint-826/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KoBert/checkpoint-826/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KoBert/checkpoint-826/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역. If clean, 인종/국적, 악플/욕설, 남성, 문장, 여성/가족, 성소수자, 종교, 개인지칭, 연령, 기타 혐오, 지역 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/KoBert/checkpoint-885\n",
      "Configuration saved in model_output/KoBert/checkpoint-885/config.json\n",
      "Model weights saved in model_output/KoBert/checkpoint-885/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KoBert/checkpoint-885/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KoBert/checkpoint-885/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from model_output/KoBert/checkpoint-826 (score: 0.8247177516979511).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=885, training_loss=0.2099381656970008, metrics={'train_runtime': 712.8164, 'train_samples_per_second': 315.712, 'train_steps_per_second': 1.242, 'total_flos': 1.1557313597626524e+16, 'train_loss': 0.2099381656970008, 'epoch': 15.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model_output/KoBert\n",
      "Configuration saved in model_output/KoBert/config.json\n",
      "Model weights saved in model_output/KoBert/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/KoBert/tokenizer_config.json\n",
      "Special tokens file saved in model_output/KoBert/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "pipe = TextClassificationPipeline(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    device=0,\n",
    "    return_all_scores=True,\n",
    "    function_to_apply='sigmoid'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': '여성/가족', 'score': 0.5228493809700012}\n",
      "{'label': '남성', 'score': 0.041127901524305344}\n",
      "{'label': '성소수자', 'score': 0.19210051000118256}\n",
      "{'label': '인종/국적', 'score': 0.04396301135420799}\n",
      "{'label': '연령', 'score': 0.030368909239768982}\n",
      "{'label': '지역', 'score': 0.02861735038459301}\n",
      "{'label': '종교', 'score': 0.03193731978535652}\n",
      "{'label': '기타 혐오', 'score': 0.06751430779695511}\n",
      "{'label': '악플/욕설', 'score': 0.06330510228872299}\n",
      "{'label': 'clean', 'score': 0.07052849978208542}\n"
     ]
    }
   ],
   "source": [
    "for result in pipe(\"포괄적차별금지법을 반대합니다.\")[0]:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicated_label(output_labels, min_score):\n",
    "    labels = []\n",
    "    for label in output_labels:\n",
    "        if label['score'] > min_score:\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disabling tokenizer parallelism, we're using DataLoader multithreading already\n",
      "100%|██████████| 3737/3737 [00:37<00:00, 100.18it/s]\n"
     ]
    }
   ],
   "source": [
    "predicated_labels = []\n",
    "\n",
    "for out in tqdm.tqdm(pipe(KeyDataset(dataset['test'], '문장'))):\n",
    "    predicated_labels.append(get_predicated_label(out, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.64      0.70       393\n",
      "           1       0.85      0.75      0.80       340\n",
      "           2       0.90      0.74      0.82       281\n",
      "           3       0.84      0.70      0.76       422\n",
      "           4       0.92      0.40      0.56       146\n",
      "           5       0.92      0.82      0.87       261\n",
      "           6       0.90      0.81      0.85       294\n",
      "           7       0.00      0.00      0.00       134\n",
      "           8       0.60      0.56      0.58       770\n",
      "           9       0.67      0.72      0.69       944\n",
      "\n",
      "   micro avg       0.75      0.66      0.70      3985\n",
      "   macro avg       0.74      0.61      0.66      3985\n",
      "weighted avg       0.74      0.66      0.69      3985\n",
      " samples avg       0.69      0.67      0.68      3985\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(classification_report(tokenized_dataset['test']['labels'], predicated_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gradproj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c8d2ad54552b88243295cd3e8f1e7e8f5e2a5dea184f85e1081eece3d09cde9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
