{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import BertForSequenceClassification, TrainingArguments, Trainer, AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "import tqdm\n",
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "from transformers.pipelines.base import KeyDataset\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KcBERT: KoBERT trained from scratch using comments data of naver news\n",
    "model_name = 'beomi/kcbert-base'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-7381e1b9c9e59c28\n",
      "Reusing dataset csv (/home/kdy20401/.cache/huggingface/datasets/csv/default-7381e1b9c9e59c28/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n",
      "100%|██████████| 2/2 [00:00<00:00, 262.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# option 1. cleaned dataset\n",
    "base_dir = os.getcwd()\n",
    "train_data = base_dir + '/dataset/processed_unsmile_train_v1.0.csv'\n",
    "test_data = base_dir + '/dataset/processed_unsmile_test_v1.0.csv'\n",
    "\n",
    "dataset = load_dataset('csv', data_files={'train': train_data, 'test': test_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration smilegate-ai--kor_unsmile-e0f75c6e3be1af78\n",
      "Reusing dataset parquet (/home/kdy20401/.cache/huggingface/datasets/parquet/smilegate-ai--kor_unsmile-e0f75c6e3be1af78/0.0.0/1638526fd0e8d960534e2155dc54fdff8dce73851f21f031d2fb9c2cf757c121)\n",
      "100%|██████████| 2/2 [00:00<00:00, 290.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# option 2. original dataset from huggingface.co\n",
    "hf_dataset = load_dataset('smilegate-ai/kor_unsmile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '개인지칭' label is excluded\n",
    "unsmile_columns = [\"여성/가족\",\"남성\",\"성소수자\",\"인종/국적\",\"연령\",\"지역\",\"종교\",\"기타 혐오\",\"악플/욕설\",\"clean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    tokenized_examples = tokenizer(str(examples[\"문장\"]))\n",
    "    tokenized_examples['labels'] = torch.tensor([examples[col] for col in unsmile_columns], dtype=torch.float)\n",
    "    # multi label classification 학습을 위해선 label이 float 형태로 변형되어야 합니다.\n",
    "    # huggingface datasets 최신 버전에는 'map' 함수에 버그가 있어서 변형이 올바르게 되지 않습니다.\n",
    "    \n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/beomi/kcbert-base/resolve/main/config.json from cache at /home/kdy20401/.cache/huggingface/transformers/10de039f2f91b0c6fbd30fad5bf8a7468a20701212ed12f9f5e610edb99c55d1.d8a72131e15fd1d856f1b39abf4eff31d458aeeca0a4192df898ca699ec7d779\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"beomi/kcbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 300,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/beomi/kcbert-base/resolve/main/vocab.txt from cache at /home/kdy20401/.cache/huggingface/transformers/527aa95c387f7c7aa3bebe490a9ede81af16f407b169db730d22632d5822b640.1b39769be8fe13da6152a54d35d7973b687b1aa6067771885d39610963e29dbe\n",
      "loading file https://huggingface.co/beomi/kcbert-base/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/beomi/kcbert-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/beomi/kcbert-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/beomi/kcbert-base/resolve/main/tokenizer_config.json from cache at /home/kdy20401/.cache/huggingface/transformers/21078f0099ac15db7a5163f4fea7f742808c30cb0393f1ee56a43dc56d9eb082.cca45b9490565b45e1c62cf5a0529b670fc5ab0db2d4a4af99f6ac577b673eb1\n",
      "loading configuration file https://huggingface.co/beomi/kcbert-base/resolve/main/config.json from cache at /home/kdy20401/.cache/huggingface/transformers/10de039f2f91b0c6fbd30fad5bf8a7468a20701212ed12f9f5e610edb99c55d1.d8a72131e15fd1d856f1b39abf4eff31d458aeeca0a4192df898ca699ec7d779\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"beomi/kcbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 300,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/beomi/kcbert-base/resolve/main/config.json from cache at /home/kdy20401/.cache/huggingface/transformers/10de039f2f91b0c6fbd30fad5bf8a7468a20701212ed12f9f5e610edb99c55d1.d8a72131e15fd1d856f1b39abf4eff31d458aeeca0a4192df898ca699ec7d779\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"beomi/kcbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 300,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "100%|██████████| 15003/15003 [00:03<00:00, 3864.03ex/s]\n",
      "100%|██████████| 3737/3737 [00:00<00:00, 3929.67ex/s]\n"
     ]
    }
   ],
   "source": [
    "# tokenize data\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=False)\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    2,  2458, 15751, 24930, 24351, 29278, 17038, 11631,     3]),\n",
       " 'labels': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/beomi/kcbert-base/resolve/main/config.json from cache at /home/kdy20401/.cache/huggingface/transformers/10de039f2f91b0c6fbd30fad5bf8a7468a20701212ed12f9f5e610edb99c55d1.d8a72131e15fd1d856f1b39abf4eff31d458aeeca0a4192df898ca699ec7d779\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 300,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/beomi/kcbert-base/resolve/main/pytorch_model.bin from cache at /home/kdy20401/.cache/huggingface/transformers/1c204bf1f008ee734eeb5ce678b148d14fa298802ce16d879c92a22a52527a0e.6cdf570ee57a7f6a5c727c436a4c26d8e9601ddaa1377ebcb16b7285d76125cd\n",
      "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(unsmile_columns)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=num_labels, # original number of labels of BERT is two(0, 1)\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "model.config.id2label = {i: label for i, label in zip(range(num_labels), unsmile_columns)}\n",
    "model.config.label2id = {label: i for i, label in zip(range(num_labels), unsmile_columns)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'여성/가족': 0,\n",
       " '남성': 1,\n",
       " '성소수자': 2,\n",
       " '인종/국적': 3,\n",
       " '연령': 4,\n",
       " '지역': 5,\n",
       " '종교': 6,\n",
       " '기타 혐오': 7,\n",
       " '악플/욕설': 8,\n",
       " 'clean': 9}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(x):\n",
    "    return {\n",
    "        'lrap': label_ranking_average_precision_score(x.label_ids, x.predictions),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# before feeding batch data to the model, apply padding to each sequences in the batch to fit the tensor size\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"model_output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='lrap',\n",
    "    greater_is_better=True,\n",
    "    optim='adamw_torch'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=args, \n",
    "    train_dataset=tokenized_dataset[\"train\"], \n",
    "    eval_dataset=tokenized_dataset[\"test\"], \n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: clean, 여성/가족, 개인지칭, 악플/욕설, 인종/국적, 성소수자, 문장, 연령, 기타 혐오, 지역, 종교, 남성. If clean, 여성/가족, 개인지칭, 악플/욕설, 인종/국적, 성소수자, 문장, 연령, 기타 혐오, 지역, 종교, 남성 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 15003\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 295\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='295' max='295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [295/295 02:11, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Lrap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.241682</td>\n",
       "      <td>0.768286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.169205</td>\n",
       "      <td>0.855768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.151370</td>\n",
       "      <td>0.866659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.144302</td>\n",
       "      <td>0.870486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.142209</td>\n",
       "      <td>0.873362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: clean, 여성/가족, 개인지칭, 악플/욕설, 인종/국적, 성소수자, 문장, 연령, 기타 혐오, 지역, 종교, 남성. If clean, 여성/가족, 개인지칭, 악플/욕설, 인종/국적, 성소수자, 문장, 연령, 기타 혐오, 지역, 종교, 남성 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/checkpoint-59\n",
      "Configuration saved in model_output/checkpoint-59/config.json\n",
      "Model weights saved in model_output/checkpoint-59/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/checkpoint-59/tokenizer_config.json\n",
      "Special tokens file saved in model_output/checkpoint-59/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: clean, 여성/가족, 개인지칭, 악플/욕설, 인종/국적, 성소수자, 문장, 연령, 기타 혐오, 지역, 종교, 남성. If clean, 여성/가족, 개인지칭, 악플/욕설, 인종/국적, 성소수자, 문장, 연령, 기타 혐오, 지역, 종교, 남성 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/checkpoint-118\n",
      "Configuration saved in model_output/checkpoint-118/config.json\n",
      "Model weights saved in model_output/checkpoint-118/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/checkpoint-118/tokenizer_config.json\n",
      "Special tokens file saved in model_output/checkpoint-118/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: clean, 여성/가족, 개인지칭, 악플/욕설, 인종/국적, 성소수자, 문장, 연령, 기타 혐오, 지역, 종교, 남성. If clean, 여성/가족, 개인지칭, 악플/욕설, 인종/국적, 성소수자, 문장, 연령, 기타 혐오, 지역, 종교, 남성 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/checkpoint-177\n",
      "Configuration saved in model_output/checkpoint-177/config.json\n",
      "Model weights saved in model_output/checkpoint-177/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/checkpoint-177/tokenizer_config.json\n",
      "Special tokens file saved in model_output/checkpoint-177/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: clean, 여성/가족, 개인지칭, 악플/욕설, 인종/국적, 성소수자, 문장, 연령, 기타 혐오, 지역, 종교, 남성. If clean, 여성/가족, 개인지칭, 악플/욕설, 인종/국적, 성소수자, 문장, 연령, 기타 혐오, 지역, 종교, 남성 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/checkpoint-236\n",
      "Configuration saved in model_output/checkpoint-236/config.json\n",
      "Model weights saved in model_output/checkpoint-236/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/checkpoint-236/tokenizer_config.json\n",
      "Special tokens file saved in model_output/checkpoint-236/special_tokens_map.json\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: clean, 여성/가족, 개인지칭, 악플/욕설, 인종/국적, 성소수자, 문장, 연령, 기타 혐오, 지역, 종교, 남성. If clean, 여성/가족, 개인지칭, 악플/욕설, 인종/국적, 성소수자, 문장, 연령, 기타 혐오, 지역, 종교, 남성 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output/checkpoint-295\n",
      "Configuration saved in model_output/checkpoint-295/config.json\n",
      "Model weights saved in model_output/checkpoint-295/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/checkpoint-295/tokenizer_config.json\n",
      "Special tokens file saved in model_output/checkpoint-295/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from model_output/checkpoint-295 (score: 0.87336236519405).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=295, training_loss=0.1932741456112619, metrics={'train_runtime': 131.7882, 'train_samples_per_second': 569.209, 'train_steps_per_second': 2.238, 'total_flos': 2787445072114668.0, 'train_loss': 0.1932741456112619, 'epoch': 5.0})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model_output\n",
      "Configuration saved in model_output/config.json\n",
      "Model weights saved in model_output/pytorch_model.bin\n",
      "tokenizer config file saved in model_output/tokenizer_config.json\n",
      "Special tokens file saved in model_output/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "pipe = TextClassificationPipeline(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    device=0,\n",
    "    return_all_scores=True,\n",
    "    function_to_apply='sigmoid'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': '여성/가족', 'score': 0.0744129866361618}\n",
      "{'label': '남성', 'score': 0.018040314316749573}\n",
      "{'label': '성소수자', 'score': 0.31230801343917847}\n",
      "{'label': '인종/국적', 'score': 0.023305071517825127}\n",
      "{'label': '연령', 'score': 0.050473958253860474}\n",
      "{'label': '지역', 'score': 0.06035453826189041}\n",
      "{'label': '종교', 'score': 0.01604899950325489}\n",
      "{'label': '기타 혐오', 'score': 0.1687774360179901}\n",
      "{'label': '악플/욕설', 'score': 0.15697471797466278}\n",
      "{'label': 'clean', 'score': 0.0743395984172821}\n"
     ]
    }
   ],
   "source": [
    "for result in pipe(\"포괄적차별금지법을 반대합니다.\")[0]:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicated_label(output_labels, min_score):\n",
    "    labels = []\n",
    "    for label in output_labels:\n",
    "        if label['score'] > min_score:\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3737/3737 [00:34<00:00, 108.38it/s]\n"
     ]
    }
   ],
   "source": [
    "predicated_labels = []\n",
    "\n",
    "for out in tqdm.tqdm(pipe(KeyDataset(dataset['test'], '문장'))):\n",
    "    predicated_labels.append(get_predicated_label(out, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.75      0.78       393\n",
      "           1       0.90      0.78      0.84       340\n",
      "           2       0.88      0.77      0.82       281\n",
      "           3       0.86      0.75      0.80       422\n",
      "           4       0.90      0.77      0.83       146\n",
      "           5       0.88      0.85      0.87       261\n",
      "           6       0.88      0.89      0.88       294\n",
      "           7       0.00      0.00      0.00       134\n",
      "           8       0.76      0.63      0.69       770\n",
      "           9       0.78      0.72      0.75       944\n",
      "\n",
      "   micro avg       0.83      0.71      0.77      3985\n",
      "   macro avg       0.77      0.69      0.73      3985\n",
      "weighted avg       0.80      0.71      0.75      3985\n",
      " samples avg       0.75      0.73      0.73      3985\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/kdy20401/anaconda3/envs/gradproj/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(classification_report(tokenized_dataset['test']['labels'], predicated_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52634da84371cba311ea128a5ea7cdc41ff074b781779e754b270ff9f8153cee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
